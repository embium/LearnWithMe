-> [YouTube Video Link](https://www.youtube.com/watch?v=gt52wUN3VrQ&list=PLUl4u3cNGP61I4aI5T6OaFfRK2gihjiMm&index=20&pp=iAQB)

### I. Introduction to Recurrent Neural Networks
#### A. Overview of Simple Recurrent Networks

Recurrent neural networks (RNNs) have been extensively explored for their ability to perform computations based on recurrent connections between neurons. The simplest form of a recurrent network is a single neuron with a recurrent connection, described by an equation that includes a leak term and an additional input proportional to the firing rate of the neuron.

#### B. Equation and Behavior

The behavior of this network depends strongly on the value of lambda (Î»), which determines whether the firing rate relaxes exponentially toward zero or grows exponentially. Three cases were discussed:

*   **Lambda less than 1**: The firing rate decays exponentially.
*   **Lambda equal to 1**: The firing rate integrates the input and ramps up linearly.
*   **Lambda greater than 1**: The firing rate grows exponentially, useful for storing memories as the network remembers that there was an input even after it's gone.

### II. Stabilizing the Network
#### A. Introduction of Nonlinearities

To solve the problem of neurons with firing rates running away exponentially, nonlinearities in the firing curve were introduced, allowing the network to have stable fixed points and attractors.

#### B. Energy Landscape Concept

The concept of an energy landscape was also discussed, where the network can be described as a ball on a hill, rolling toward one of two stable fixed points. This idea was extended to multiple neurons, where each neuron has its own saturation point, and the network can implement decision-making by starting at zero and being kicked into one of two attractors based on input.

### III. The Hopfield Model
#### A. Introduction

The Hopfield model was introduced as a formal implementation of a system for producing long-term memories, which is similar to how the hippocampus works in storing memories.

#### B. Multiple Memories Storage

A Hopfield network can store multiple memories by computing the contribution to the weight matrix from each desired pattern, then adding them together. This creates a network where the state at time T plus one is just the sign of a times CI times the sum over J of ik c j ik c j.

### IV. Capacity and Limitations
#### A. Calculating Capacity

The capacity of a Hopfield network can be calculated by considering how many memories can be stored without them being too similar to each other. If we train a network on multiple patterns and then run it with an input that is close to one of those patterns, the system will evolve toward that pattern.

#### B. Spurious Attractors

However, if the network is trained on too many patterns, it may develop spurious attractors, which are stable states that are not actually desired memories. This can happen when the number of patterns exceeds the capacity of the network to store them without overlap between the patterns.

#### C. Crosstalk and Overlapping Memories

The capacity of a neural network is limited by crosstalk, which occurs when stored patterns are not orthogonal to each other. If memories are highly overlapping, they can become unstable attractors and interfere with one another. To prevent this, it's essential to store patterns as random or nearly orthogonal vectors.

#### D. Limitations and Local Minima

By doing so, the network can store up to 15% of its neurons' worth of memories without significant crosstalk. However, when too many memories are stored, spurious attractors can form, leading the system to get stuck in local minima that don't correspond to actual memories.

#### E. Relation to Long-term Memories

This phenomenon is thought to be related to the formation of long-term memories, which are programmed into synaptic connections and remain stable even without network activity.