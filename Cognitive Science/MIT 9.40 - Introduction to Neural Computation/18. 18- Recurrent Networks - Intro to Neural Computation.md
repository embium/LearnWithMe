-> [YouTube Video Link](https://www.youtube.com/watch?v=EpPtCLkCGOk&list=PLUl4u3cNGP61I4aI5T6OaFfRK2gihjiMm&index=18&pp=iAQB)

### I. Introduction to Recurrent Networks in Computational Neuroscience
#### A. Background and Motivation

Recurrent networks are a fundamental concept in computational neuroscience, allowing neurons to communicate with each other and imbue properties onto these networks. This topic is particularly interesting as it explores the dynamics of neural networks, enabling them to act as amplifiers, integrators, generators of sequences, and short-term memories for continuous or discrete variables.

#### B. Recap of Previous Work

Previous work in computational neuroscience has focused on rate models, where spikes are replaced with a single number characterizing the firing rate of neurons. Simple networks consisting of an input neuron and an output neuron with synaptic connections between them have been introduced. Different types of binary threshold units and linear neurons have also been discussed.

#### C. Key Concepts

*   **Recurrent Networks**: A type of neural network where not only are there inputs to output neurons from an input layer but also connections between neurons in the output layer, allowing them to communicate with each other.
*   **Amplifiers, Integrators, Generators, and Short-term Memories**: Recurrent networks can act as amplifiers of particular directions, integrators that accumulate information over time, generators of sequences, and short-term memories for continuous or discrete variables.

### II. Mathematical Description of Recurrent Networks
#### A. Introduction to Linear Algebra Tools

The mathematical description of recurrent networks involves dynamics in these networks, starting with the simplest case of an autapse (a neuron connected to itself) and extending to general recurrent connectivity. Linear algebra tools will be used to connect with previous developments.

#### B. Diagonalizing the Weight Matrix

To simplify the equation for the fully recurrent network, we can diagonalize the weight matrix M by rewriting it in a new form where lambda is a diagonal matrix. The elements of lambda are the eigenvalues of M, while the columns of phi are the eigenvectors of M.

#### C. Simplifying the Equation

By rewriting the vector of firing rates V in this new basis set, we can simplify the equation into tau DC DT equals lambda C plus H, where h is just h rotated into the new basis system. This simplifies the equation into a form that allows us to understand the behavior of each mode separately.

#### D. Understanding Network Behavior

We've taken a complicated network with recurrent connections and rewritten it in a new network where each neuron corresponds to what's called a mode of the fully recurrent network. The activities C alpha of the network modes represent an activity in a linear combination of these neurons.

### III. Steady-state Responses and Network Behavior
#### A. Response to Inputs

By calculating the steady-state responses, we can see that if an input is parallel to one of the eigenvectors of the weight matrix, the output will be parallel to the input with a scaling factor 1 over 1 minus lambda.

#### B. Modes and Network Behavior

The response of the network will be in the direction of that input and will be amplified or suppressed by this gain factor. We've also seen how to calculate the steady-state responses for different inputs and how the network's behavior changes depending on the sign of lambda.

### IV. Limitations and Future Directions
#### A. Exponential Growth and Saturating Nonlinearities

However, this model has limitations, as exponentially growing activity would be detrimental to neural networks. To address this, researchers use saturating nonlinearities, which allow neurons to initially grow but eventually reach a maximum firing rate, stabilizing the activity at a high value.

#### B. Implications for Neural Networks and Memory Storage

This type of neuron can remember inputs and is similar to how the hippocampus stores memories through recurrent connections that activate each other before saturating.